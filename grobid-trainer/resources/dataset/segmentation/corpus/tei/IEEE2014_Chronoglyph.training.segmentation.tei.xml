<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front> A Visual Analytical Approach for Model Editing and Testing in <lb/>Glyph-based Time Series Compression <lb/> Eamonn Maguire, Student Member, IEEE, Susanna-Assunta Sansone, and Min Chen, Member, IEEE <lb/> Abstract— Time series data is ubiquitous in the present world. In many applications, e.g., finance, meteorology, biology, chemistry, <lb/>physics and engineering, such data is constantly being recorded. It is highly desirable to speed up the process of viewing time series <lb/>data by exploring new visual representations to complement the conventional line graph that was invented over a millennium ago. One <lb/>existing methodology is to detect Frequent Long Patterns (FLPs) and replace them with short and abstract notations, such as glyphs. <lb/>In many applications, the deployment of this methodology has encountered the familiar problem of separating noise from anomalous <lb/>features. On the one hand, the detection algorithm needs to tolerate a fair amount of noise to facilitate a high compression ratio. <lb/>On the other hand, the detection algorithm needs to avoid any false positive at all cost. In this paper, we propose a visual analytics <lb/>approach to address this paradoxical problem. We first use the conventional FLP detection algorithm to create a rough model that <lb/>places its emphasis on noise tolerance. This initial model is then used to identify a set of FLPs in a corpus of time series, which <lb/>serve as the testing results of the model. We transform this set of test FLPs to a parameter space, allowing for in detail analysis using <lb/>parallel coordinates and network plots. Interactive visualization enables users to identify as many false positive results as possible, <lb/>and more importantly, to identify features that can be used to filter them out. The brushing interaction with the parallel coordinates is <lb/>then used to refine the initial model by activating feature-based filtering instructions in a model editing window. This approach can be <lb/>iteratively applied to a model being developed. We have implemented a prototype system to demonstrate this novel approach. <lb/></front>

			<body> 1 INTRODUCTION <lb/> In many applications, such as finance, meteorology, biology, chem-<lb/>istry, physics and engineering, users have collected a large volume of <lb/>time series data. A time series corpus T is a collection of time series <lb/>that record phenomena of a similar nature. On the one hand, users of-<lb/>ten find that it is time consuming and cognitively demanding to browse <lb/>many time series in a time series corpus. On the other hand, the cor-<lb/>pus provides an opportunity to identify frequently occurring patterns. <lb/>When such patterns are of a reasonable length, it is advantageous to <lb/>replace them with a short abstract representation, such as a text label, a <lb/>signature pattern, a glyph or a combination of these (e.g., [18]). This is <lb/>a form of visual compression, which is commonly seen in real life. For <lb/>example, in signage management, frequently encountered restrictions <lb/>and events are encoded as signs and icons, while those of an occa-<lb/>sional or exceptional nature are written in text. From an information-<lb/>theoretic perspective, such a strategy is fundamentally the same as that <lb/>used in entropy encoding and dictionary encoding [10, 31]. <lb/>In time series processing and visualization, the algorithm for iden-<lb/>tifying Frequent Long Patterns (FLPs) plays a critical role. It has to <lb/>be sufficiently tolerant to noise that causes variations to the patterns <lb/>in the corpus. Without a high-degree of noise tolerance, the criteria <lb/>of &apos;frequent&apos; and &apos;long&apos; could not be met, and the objective of visual <lb/>compression could not be fulfilled. However, paradoxically the FLP <lb/>algorithm is also required to exclude any anomalous patterns, which <lb/>would easily be mistaken as frequently occurring patterns once they <lb/>are visually compressed. In many situations, an anomalous pattern <lb/>differs from frequent occurring patterns in a subtle way, and a con-<lb/>ventional FLP algorithm may not handle the features that characterize <lb/>such differences. <lb/>Fig. 1(A) shows six segments of a time series. They all appear to be <lb/></body>

			<front> • Eamonn Maguire is with the Oxford e-Research Centre and Department of <lb/>Computer Science, University of Oxford. E-mail: <lb/>eamonn.maguire@st-annes.ox.ac.uk. <lb/> • Susanna-Assunta Sansone is with the Oxford e-Research Centre, <lb/>University of Oxford. E-mail: sa.sansone@gmail.com. <lb/> • Min Chen is with the Oxford e-Research Centre, University of Oxford. <lb/>E-mail: min.chen@oerc.ox.ac.uk. <lb/>Manuscript received 31 March 2014; accepted 1 August 2014; posted online <lb/>13 October 2014; mailed on 4 October 2014. <lb/>For information on obtaining reprints of this article, please send <lb/>e-mail to: tvcg@computer.org. <lb/></front>

			<body> fairly similar, and a typical FLP algorithm (Fig. 1(B)) would consider <lb/>them to be the same. However, the 4th segment is an anomaly. An <lb/>ideal visual compression should not replace this segment with a glyph, <lb/>while compressing as many others as possible, as illustrated in Fig. <lb/>1(C). <lb/>In this paper, we present a visual analytics approach to address this <lb/>paradoxical conflict of requirements upon an FLP algorithm. Consider <lb/>that an FLP identified by an FLP algorithm is a model. The basic idea <lb/>is to add an additional filtering capability to this model. As the filtering <lb/>does not take place within the parameter space of the original FLP <lb/>algorithm, it can handle features that the FLP algorithm cannot. The <lb/>function of visual analytics is to support the following in an iterative <lb/>manner: <lb/> • discover any anomalous patterns that may have been included by <lb/>an FLP model; <lb/> • compute a bags of features that may be used to characterize dif-<lb/>ferent time series segments identified by an FLP model, and to <lb/>visualize them in the feature space; <lb/> • assist users in a number of analytical tasks for identifying the <lb/>appropriate filters, e.g., results clustering and tagging, and ob-<lb/>serving the effects of feature selection on filtering results; and <lb/> • facilitate model editing by transforming interaction in visualiza-<lb/>tion to text-based instructions in the FLP model. <lb/>In the remainder of the paper, we first give a brief overview of time <lb/>series analysis and visualization in Section 2. In Section 3 we outline <lb/>a visual analytics loop for analyzing, visualizing, testing, and editing <lb/>an FLP model, and we describe a prototype system that supports such <lb/>a visual analytics loop. In Section 4, we describe an FLP algorithm <lb/>that serves as the core component of an FLP model. In Section 5, we <lb/>describe a collection of features that have been implemented to support <lb/>model analysis and editing. In Section 6 we describe the model editing <lb/>system and introduce the logical operators that can be used to combine <lb/>models as well as build up rules for parameter refinement. Finally, in <lb/>Section 7 we present a visualization to visualize the results of the time <lb/>series compression. <lb/> 2 RELATED WORK <lb/> The related work can be divided in to &apos;time series analysis&apos;, focusing <lb/>specifically on &apos;normalization&apos;, &apos;approximation&apos; and &apos;similarity&apos;, and <lb/>&apos;time series visualizations&apos;. <lb/> B) <lb/> A) <lb/>C) <lb/> 1.09 <lb/> -3.10 <lb/>7.06 <lb/>1.67 <lb/>1.52 <lb/>0.97 <lb/>2.28 <lb/>1.26 <lb/>-0.18 <lb/>4.02 <lb/>1.73 <lb/>1.37 <lb/>0.80 <lb/>1.78 <lb/>1.12 <lb/>-0.18 <lb/>4.02 <lb/>1.69 <lb/>1.51 <lb/>0.96 <lb/>2.09 <lb/>1.17 <lb/>-0.22 <lb/>4.34 <lb/>1.76 <lb/>1.50 <lb/>0.98 <lb/>2.14 <lb/>0.96 <lb/>-3.10 <lb/>7.06 <lb/>1.58 <lb/>1.64 <lb/>1.14 <lb/>2.71 <lb/>1.72 <lb/>-0.18 <lb/>4.02 <lb/>1.70 <lb/>1.45 <lb/>0.90 <lb/>1.98 <lb/> Fig. 1. A) Six time series subsections identified in a time series corpus <lb/>of space shuttle valve time function taken from [27] B) Symbolic rep-<lb/>resentation of each time results in all motifs being seen as the same, <lb/>however their more detailed metrics/features differ C) Leaving out the <lb/>anomalous pattern (A4) from the compression as requested by a user <lb/>means that the final time series compresses the actual common motifs <lb/>and leaves the anomalous pattern in plain view. <lb/> 2.1 Time Series Analysis <lb/> Time series analysis encompasses five main sub-categories of opera-<lb/>tions: indexing, clustering, classification, summarization and anomaly <lb/>detection [34]. <lb/> Indexing: This is to provide support to storing, searching and <lb/>retrieving time series in relation to a data repository. <lb/> Clustering: This is to determine how a time series compares with <lb/>other time series in a database. <lb/> Classification: Given a predefined categorization, this is to determine <lb/>if a new time series belongs to a specific class. <lb/> Summarization: Given a large time series dataset, this is to summa-<lb/>rize the time series and/or its main attributes using a more compact <lb/>representation (e.g., a feature vector or a visualization). <lb/> Anomaly detection: Given some time series, highlight anomalous <lb/>incidents based on the profiles of a set of normal events (e.g., [28]). <lb/>For all of the above tasks, computing the similarity of between time <lb/>series data is key. This area of research has a large amount of literature <lb/>dedicated to it. Here we focus on some of the most relevant and com-<lb/>monly used algorithms that lend themselves to the task of time series <lb/>comparison, both for whole and subsequence comparison. <lb/> Normalization. Before comparing two or more time series, an ad-<lb/>justment may need to be made to the data sets so that the data itself is <lb/>comparable. For example, if we are looking for commonality in trends, <lb/>the similarity score should be comparing the overall topology between <lb/>points, rise and falls rather than being too concerned with where on <lb/>the y-axis those trends reside. This step is called normalization. One <lb/>of the most common approaches used for this step is Z-normalization, <lb/>where the time series are manipulated to have a mean of zero and a <lb/>standard deviation of one. <lb/> Approximation. Following normalization, we may wish to reduce <lb/>the dimensionality of the data. This means that instead of compar-<lb/>ing a time series in terms of real data points, we compare approxi-<lb/>mations that are smaller in size but largely retain the important fea-<lb/>tures of the series. There are numerous techniques available for this <lb/>process, all with their own faults and merits. Here we focus on the <lb/>most relevant and commonly used techniques for approximation of <lb/>a time series, those being: Discrete Fourier Transformation (DFT) <lb/>[12] is an algorithm capable of representing the overall &apos;features&apos; of <lb/>a time series through spectral decomposition. This means that the sig-<lb/>nal represented by the time series is decomposed into a series of sine <lb/>(and/or cosine) waves each represented by a Fourier coefficient [24] <lb/>-this proves to be a very efficient way of compressing data; Discrete <lb/>Wavelet Transformation (DWT) [8] is an algorithm similar in prin-<lb/>ciple to DFT, however it has one key difference in that some of the <lb/>wavelet coefficients represent small local regions of the series mean-<lb/>ing that wavelets lend themselves to providing a multi-resolution view <lb/>of a time series. The results of DWT do not lend themselves to being <lb/>indexed (for comparison purposes). However an extension called the <lb/>Haar Wavelet can be calculated efficiently and it&apos;s outputs can be in-<lb/>dexed for efficient time series matching [7]. The key drawback with <lb/>DWT is that the length of the time series must be an integral power <lb/>of two [24]; Piecewise Linear Appromixation (PLA) [6] or Segmented <lb/>Regression is an algorithm that breaks a time series up in to &apos;windows&apos;, <lb/>then represents that window with a line segment that best fits the val-<lb/>ues in that window; Piecewise Aggregate Approximation (PAA) [24] <lb/>works by splitting a time series up in to &apos;windows&apos; where each window <lb/>contains one or more time points. For each window, the average of the <lb/>time points within is recorded and stored in a vector. The approach is <lb/>simple yet is shown to rival DFT and DWT [34, 26]; Adaptive Piece-<lb/>wise Constant Approximation (APCA) [25] is algorithmically similar <lb/>to PAA with extensions that further compress the representation using <lb/>a technique common to data compression known as run-length encod-<lb/>ing (RLE). This extension is the addition of a second number beside <lb/>the mean value of the window indicating the length of the segment; <lb/>and Symbolic Aggregate Approximation (SAX) [34, 35] which builds <lb/>on PAA to take the average values calculated for each window then <lb/>assigns a letter to that window based on where the mean value falls <lb/>under a Gaussian curve. The result is a symbolic representation of a <lb/>time series that lends itself to many computational manipulations such <lb/>as hashing or storage in data structures such as suffix trees for fast <lb/>pattern searching. Suffix trees have been used by Lin et al [34], to <lb/>build a motif discovery tool for time series data, capable of highlight-<lb/>ing anomalous data. A weakness of the symbolic approach however <lb/>is the need to define a window and alphabet size ahead of time -these <lb/>will differ depending on the domain due to differences in periodicity, <lb/>variance for example. <lb/> Similarity. Following these steps, comparison can be performed on <lb/>the outputs of the approximation which will give an idea of how close <lb/>a time series is to another. Starting with the most simple of methods, <lb/>there is Euclidean distance, where for two time series (t 1  ,t  2 ) the Eu-<lb/>clidean distance would be calculated using <lb/> ∑ <lb/> n <lb/>k=1  (t  1  k  − t  2  k  ) where t n k <lb/> represents a point in time series n. This metric is fast to compute due <lb/>to its simplicity, however it carries the caveat that the two sequences <lb/>must be of the same length. When sequences are not of the same <lb/>length, there are more complicated algorithms largely based on dy-<lb/>namic programming methodologies such as: Dynamic Time Warping <lb/>[4] that allow for comparison between sequences of varying lengths <lb/>and also support shifts in the series; Edit distance with Real Penalty <lb/>(ERP) allows for gaps in a time series that may be penalised using <lb/>some configurable value; Longest Common Subsequence (LCSS) [43] <lb/>introduces a threshold value allowing a degree of mismatch between <lb/>sequences; and Edit Distance on Real sequences (EDR) [9] merges <lb/>the concept of gaps and mismatch thresholds presented in ERP and <lb/>LCSS respectively. Due to the use of dynamic programming tech-<lb/>niques, DTW, ERP, LCSS and EDR have limitations in that there are <lb/>many comparisons made between sequences that have no relevance <lb/>whatsoever. To address these limitations, there is the Fast Time Se-<lb/>ries Evaluation [37] algorithm that introduced a more efficient way of <lb/>building up the comparison matrix meaning that non-related sequences <lb/>were never compared. <lb/> 2.2 Time Series Visualization <lb/> Time series visualizations, given their use across every possible field <lb/>have been published about heavily in the past two decades. More com-<lb/>plete surveys of time series visualization, can be found in [2, 13]. Here <lb/>we highlight some of the work in the visualization domain that aims to <lb/>ease the task of time series analysis and provide a more effective means <lb/>for users to navigate their data. Such work includes: StackZooming <lb/>[21] which provides a hierarchical zooming interface for time series <lb/>data; a spectral visualisation system for visualizing overviews of fi-<lb/>nancial data [23]; the use of &apos;lenses&apos; to focus on particular areas of <lb/>a time-series [30, 48]; LiveRAC [36] for computer/information sys-<lb/>tem management; Horizon charts[19] that attempt to aid comparison <lb/>of many time series in one height fixed display; TimeSearcher [20] and <lb/>VizTree [33] that allow for exploration of time series via a visual query <lb/>interface; spiral visualizations to allow for trend discovery in datasets <lb/>[45, 11]; importance-driven layouts for time series data [16]; multi-<lb/>resolution techniques for exploration of time-series data [17]; and the <lb/>visual exploration of frequent patterns in time series data [18]. <lb/>Glyphs have been used for visualization in time series for a number <lb/>of tasks. Many such uses have been in order to summarise a series <lb/>[29, 15, 44, 47]. They have also been used to represent uncertainty in <lb/>time series data [3]. <lb/> 3 A VISUAL ANALYTICS APPROACH <lb/> Fig. 2 shows an overall pipeline for detecting FLPs in a time corpus, <lb/>and for compressing a time series with glyphs. Steps 1, 2, 5 and 6 <lb/>represent the pipelines traditionally used in the literature. Steps 3 and <lb/>4 represent the newly added visual analytics steps for model testing, <lb/>visualization, analysis and editing. <lb/>Similar to the visual analytics system by Legg et al [32] for sports <lb/>video search refinement, this system will allow users to refine FLP <lb/>models resulting from a conventional FLP algorithm so that users may <lb/>filter the FLPs most appropriate for their use case. <lb/>This pipeline is detailed as follows: <lb/>1. a Time Series Corpus as input to the system -this is usually a <lb/>very large collection of time series. <lb/>2. a Frequent Long Pattern (FLP) detection subsystem -this de-<lb/>tects FLPs in the time series, the output of which is a list of <lb/>different FLP candidates. A user can choose a specific FLP as an <lb/>individualized FLP model. The model will detect only patterns <lb/>that match this FLP; <lb/>3. a Visual Analytics Platform consisting of a number of com-<lb/>ponents to aid model output view, edit and test and refinement <lb/>operations. The interface that realizes this platform is shown in <lb/>Fig. 2 A,B,C and D. It is divided in to four linked panels serving <lb/>a number of complementary functionalities. These are: <lb/> • A) Model Debugging Panel: results from the FLP model <lb/>are presented in this overview area where results of the <lb/>model can be approved or rejected. The network view <lb/>presents each motif as a glyph representing the approxi-<lb/>mation amongst its feature space. These glyphs can be <lb/>arranged in a number of different topologies: <lb/>(a) by distance between the symbolic representation of <lb/>each motif -this is calculated on motifs with equal <lb/>lengths through their euclidean distance. This is de-<lb/>fined by the equation below from Lin et al [35] <lb/> MINDIST (S  X  , S Y  ) = <lb/> n <lb/>w <lb/> w <lb/> ∑ <lb/> i=1 <lb/> (dist)(S  X i  , S Y i  )  2 <lb/> (1) <lb/>where the dist function indicates the use of a lookup <lb/>table to determine the distance between two letters, <lb/>say A and D in the symbolic approximation. Since A <lb/>and D are further away from each other than A and B <lb/> A. Node Detail <lb/>B. Parameter Reenement <lb/> Fig. 3. A) Node detail view showing the feature space of the nodes, <lb/>the approximation and the corresponding parts of the time series that <lb/>this motif is associated with. Additionally, the context is shown for the <lb/>motif in the time series panel on hovering over a node. B) On brushing <lb/>an axis in the parallel coordinate plot, a parameter refinement window <lb/>appears to allow the user to either remove results based on particular <lb/>parameters or to accept them <lb/> for each, dist(A, D) would return a larger value than <lb/> dist(A, B) <lb/> (b) by whether motifs have been accepted or not; or <lb/>(c) by a hierarchy representing parent-child relationships <lb/>where parent motifs (e.g., abbaca) have a sequence <lb/>that contains N child motifs (e.g., abb, bb, bba, or <lb/>baca) <lb/>This view also provides users with a way to select a glyph <lb/>and view more information about the motif it represents. <lb/>A popup shown in Fig. 3A allows users to mark motifs <lb/>as accepted or rejected, view their feature space in more <lb/>detail, obtain their context in the collection of time series <lb/>being analysed (see Fig. 2 B) and view the original time <lb/>series. <lb/> • B) Time Series Overview Panel: provides a summary <lb/>view of the time series being used by the motif finding al-<lb/>gorithm. This is also used to highlight where motifs occur <lb/>within the time series to provide further contextual infor-<lb/>mation to users. <lb/> • C) Model and Parameter Editing Panel: split in to two <lb/>sections: the model area -this area allows users to cre-<lb/>ate new models, changing the window ω and alphabet size <lb/> α and combine the results of these models; and the re-<lb/>finement, providing an interface for users to edit the fea-<lb/>ture ranges required for acceptance or rejection of a motif. <lb/>Users can recalculate at any point and see the results up-<lb/>date in Fig. 2 A. See Section 6. <lb/> • D) Feature Space Panel: to visualize the feature space <lb/>of the motifs, we use parallel coordinate plots. Users can <lb/>brush a combination of axes to find the &apos;best&apos; parameters <lb/>that yield the motifs they really want. This is aided by <lb/>a refinement window shown in 3 B and a link with the <lb/>network view which filters out nodes that have parameters <lb/>outside the brushed region(s) (see 2A and D). See Section <lb/>5. <lb/>4. a Refinement Loop which allows re-runs of the FLP detection <lb/>model(s) given user-defined refinements to the conditions re-<lb/>quired for motif acceptance; <lb/>5. a collection of Accepted Motifs; and <lb/> 2. FLP <lb/>Detection <lb/>Subsystem <lb/>FLP Model <lb/>Feature <lb/>Space <lb/>3. Visual Analytics <lb/>Platform <lb/> 0.5 &lt; P1 &lt; .8 <lb/>Skewedness <lb/> Model <lb/>Editing <lb/>1. Time Series <lb/>Corpus <lb/> A. Model Debugging Panel <lb/>D. Glyph Feature Space View <lb/>B. Time Series Overview Panel <lb/>C. Model &amp; Parameter Editing Panel <lb/> 5. FLP <lb/>Dictionary <lb/> 4. Reenement Loop <lb/> 6. Insertion in <lb/>time series <lb/> Overview <lb/> 4. Refinement <lb/> Fig. 2. The overview of the process followed in this paper and a screenshot of the visual analytics platform built to facilitate this process. The visual <lb/>analytics platform interface has four interlinked panels to support: A) visual model debugging; B) overview of the larger time series to support <lb/>context views for each motif (where it occurs in the actual time series); C) model and parameter editing; and D) navigation and refinement of the <lb/>feature space for each motif. <lb/> 6. a Time Series Compression step which inserts the motifs as <lb/>glyphs in to time series for compression and/or anomaly detec-<lb/>tion (see Section 7). <lb/> 4 FLP DETECTION SUBSYSTEM <lb/> In data compression, dictionary encoding is a family of methods that <lb/>search a text for strings from a dictionary, and replace the matching <lb/>strings with the corresponding codewords defined in the dictionary. <lb/>The same dictionary is maintained by the encoder and the decoder, fa-<lb/>cilitating faithful translation from the original text to the compressed <lb/>representation, and back to the original text. The primary goal of such <lb/>a compression strategy is to reduce storage requirements. In a less <lb/>algorithmic form, we apply a similar approach in writing by using ab-<lb/>breviations (e.g., UN for United Nations and symbols (e.g.,  § and c <lb/> )). <lb/> Such a &apos;compression&apos; method can be based on a general &apos;dictionary&apos; <lb/>applicable in a wide context (e.g., commonly-used currency signs), a <lb/>&apos;dictionary&apos; that is meaningful only to a specific group of people or in <lb/>a specific context (e.g., mathematical symbols, abbreviations for mea-<lb/>surement units), or an ad hoc &apos;dictionary&apos; to address a very specific <lb/>requirement (e.g., we introduced FLP as an abbreviation for Frequent <lb/>Long Pattern). The primary goal is to reduce the time and cognitive <lb/>load required for reading long and frequently-occurring text strings, <lb/>while it also facilitates space saving in many cases. <lb/>The condition of &apos;long and frequently occurring&apos; reflects the prin-<lb/>ciple of entropy encoding in information theory [10]. The most well-<lb/>known entropy encoding technique is Huffman coding. In everyday <lb/>life, we can observe many intuitive entropy encoding schemes. For <lb/>example, traffic signs are designed to speed up readings of restric-<lb/>tions and instructions, which otherwise would require a few words or <lb/>sentences. For less commonly used restrictions and instructions, we <lb/>still have text-based traffic sign boards. The balance between icon-<lb/>or glyph-based and text-based visual representations in traffic signage <lb/>features considerations of frequency of usage, length and complexity <lb/>of the original text, importance, and various human factors (e.g., learn-<lb/>ability, recognition, and memorization). <lb/>This has inspired us to apply the concepts of entropy encoding and <lb/> Aggregation <lb/> Unit <lb/>Encoding <lb/> abbcbabbbbbbabcc <lb/> ddbabbbbbbabcaba <lb/>abbbbbbabbabcbbb <lb/> Frequency <lb/>Analysis <lb/> aaccaa <lb/> OCCURRENCE <lb/> 1092 <lb/> LENGTH <lb/> 6 <lb/> ... <lb/> babc <lb/> OCCURRENCE <lb/> 765 <lb/> LENGTH <lb/> 4 <lb/> SERIES PRESENT IN <lb/> 500 <lb/> SERIES PRESENT IN <lb/> 498 <lb/> abbbbbbabbabcbbb <lb/> OCCURRENCE <lb/>LENGTH <lb/>SERIES PRESENT IN <lb/> 1. Time Series Collection <lb/>2. Create symbolic encoding <lb/>3. Find FLPs <lb/>4. Rank FLPs and <lb/>create dictionary <lb/>5. Initial FLP <lb/>Dictionary <lb/>6. Refine Selection <lb/> N <lb/> 6. View Feature Space <lb/>7. Edit Model <lb/>8. Final FLP <lb/>Dictionary <lb/> Fig. 4. Dictionary compilation steps – given a number of time series datasets (T  1  , T  2  , . . . , T n  ) (1), each is approximated by a symbolic representation <lb/>(2). A list of Frequent Long Patterns (FLPs) are found (3) from analysis of the symbolic encodings and these FLPs are ranked by their &apos;compression <lb/>potential&apos; (4). Highly-ranked FLPs are added to a dictionary. Finally, glyph representations are created for each FLP. <lb/> dictionary encoding to visualization, since they are everyday phenom-<lb/>ena and underpinned by well-established mathematical theories. We <lb/>can draw a parallel between a time series and a piece of text, and be-<lb/>tween a frequent long pattern in a collection of historical time series <lb/>and a common long string in a text corpus. This analogy suggests <lb/>that we can adopt a computational process to compile a dictionary, <lb/>since there have been many previous works on symbolic processing <lb/>of time series data in the literature (e.g., [33]). As glyphs have been <lb/>used for time series visualization (e.g., [38]), we thus focus on a vi-<lb/>sual representation that supports dictionary-based compression while <lb/>maintaining overview and selective detail views. <lb/>Fig. 4 illustrates the algorithmic steps for creating the FLP dic-<lb/>tionary for a corpus of times series data. This dictionary creation al-<lb/>gorithm is designed to process a collection of time series in order to <lb/>establish a dictionary consisting of a set of FLPs in a specific context <lb/>and can consists of the following steps: <lb/>1. Time Series Collection. It is necessary to collect a set of time <lb/>series in order to generate good statistical indications about com-<lb/>mon patterns and outliers. <lb/>2. Unit Encoding. Given a collection of time series, the next step <lb/>in Dictionary Compilation is to translate them to symbolic rep-<lb/>resentations. These symbolic representations collectively form a <lb/>&apos;text corpus&apos;. <lb/>3. Find FLPs. In this step, an efficient algorithm based on a suffix-<lb/>tree structure is deployed to detect a list of FLPs. <lb/>4. Ranking FLPs. For each detected FLP, we compute its potential <lb/>compression capacity alongside other parameters in the feature <lb/>space (see Section 5). We then choose the high capacity pat-<lb/>terns and/or those with features matching any user-defined re-<lb/>quirements defined in the visual analytics platform. <lb/>5. Creating Glyphs. The glyphs can be created automatically <lb/>based on various attributes of the time series they represent. <lb/>Consider that we have N p  FLPs in a time series, and each FLP is <lb/>of a length of M i  (i = 1, 2, . . . , N p  ) samples. If each glyph requires a <lb/>display space with a fixed width ω  g  (in pixels), the compression ratio <lb/>in terms of horizontal visual space requirement is: <lb/> C FLPs  = <lb/> ω  s  ∑ <lb/> k <lb/>i=1 M i <lb/> ω  g N p <lb/> (2) <lb/>where ω  s  is the number pixels per sample, including interval space, <lb/>along the t-axis in a conventional line graph. The overall compression <lb/>ratio for the time series is <lb/> C timeseries  = <lb/> ω  s N t <lb/> ω  s  (n − ∑ <lb/> k <lb/>i=1 M i  ) + ω  g N p <lb/> (3) <lb/>Spatial compression is merely an indicator of the potential bene-<lb/>fits of a frequency-based encoding strategy. In general, representing <lb/>parts of a time-series as appropriate glyphs can benefit some com-<lb/>mon visualization tasks, such as identifying familiar FLPs, focusing <lb/>attention on potential anomalies, and performing visual searches. The <lb/>main cost is the loss of accuracy of those glyph-encoded parts of the <lb/>time series. The trade-off is a universal mechanism at the heart of vi-<lb/>sualization. Many visualization techniques, such as zooming, glyph-<lb/>based techniques and metro-maps, feature such a trade-off. In fact, <lb/>the conventional line graphs for time-series visualization usually incur <lb/>a significant loss of accuracy in comparison with the raw data, since <lb/>the screen resolution is usually much lower than the data resolution. <lb/>With interactive visualization, the loss of accuracy in overview can be <lb/>recovered from details on demand [40], for example, using zoom or <lb/>pop-up windows. <lb/> 4.1 Unit Encoding <lb/> Let T be one of such a time series with N t  samples, we first divide <lb/>it into N unit time units, each of which consists of a fixed number of <lb/>samples. In other words, there are N spu  samples per unit, such that <lb/> N unit  = N  t  /N  spu  . In the literature, various terms have been used to <lb/>denote such a time unit, e.g., &apos;time primitive&apos; and &apos;chronon&apos; in [5] and <lb/>&apos;window&apos; in [33]. <lb/>An encoding scheme is then selected to map each unit to a symbol in <lb/>an alphabet A. In this paper, we denote such symbols with lower case <lb/>letters, such as a, b, and so on. Such an encoding scheme usually fa-<lb/>cilities an initial lossy compression in numerical representations. The <lb/>potential compression ratio is influenced primarily by the size of the <lb/>alphabet |A|, the number of samples per unit, N spu  , and the number <lb/>of bits per sample value. However, some parts of the time series will <lb/>not be encoded as glyphs and will be displayed as conventional time-<lb/>series plots instead since their points do not fall within the realms of <lb/>the frequently occurring patternss. <lb/>Being able to index and compare time series is of importance in <lb/>this work due to the necessity to determine whether or not particu-<lb/>lar patterns in a time series are different from what has already been <lb/> 1 <lb/> 8 <lb/>2 <lb/>6 <lb/>7 <lb/>3 <lb/>5 <lb/>4 <lb/>9 <lb/> Suffix Tree <lb/>Suffix Array <lb/>Symbolic Representation <lb/> Created from the <lb/>offsets present <lb/>left to right in the <lb/>suffix tree <lb/>Stores the offset <lb/>required to get <lb/>the substring. <lb/> Fig. 5. Suffix tree and array representations, adapted from [22]. <lb/> seen. In this work, we chose to base our algorithm on the symbolic ap-<lb/>proximation approach because it: 1) is widely used by the time series <lb/>community; 2) is performant with O(n) complexity; and 3) has out-<lb/>put ideally formed for indexing, hashing and storage in data structures <lb/>such as suffix trees as performed in [33]. <lb/> 4.2 Frequent Long Pattern (FLP) Detection Algorithm <lb/> From the symbolic representation calculated in Section 4.1, we need <lb/>to extract the most common, long substrings from available set of <lb/>sequences with the expectation that selection of the most common <lb/>strings will lead to greater compression. <lb/>The FLP detection model has three principle components: an &apos;en-<lb/>hanced&apos; suffix array (with longest common (LCP) prefix array) to pro-<lb/>vide a fast, space efficient platform for fast string searching; an al-<lb/>gorithm to compute the common elements in the string and their fre-<lb/>quency; and a filter to fulfil any user-defined refinements on the fea-<lb/>tures that motifs should have -by default, the model satisfies the re-<lb/>quirement of finding those patterns matching the FLP properties of <lb/>both &apos;long and frequent&apos;. <lb/> 4.2.1 Finding the FLP Candidates <lb/> A suffix tree [46, 42], or position tree is a data structure used to rep-<lb/>resent all suffixes of a string [42]. Fig. 5 is an example of such a <lb/>tree and its corresponding suffix array adapted from Kasai et al [22]. <lb/>Suffix trees thrive in their ability to speed up previously computation-<lb/>ally intensive string operations such as: finding the longest common <lb/>substring; finding repeats; finding maximal palindromes; and inexact <lb/>string matching (the Weeder algorithm). <lb/>Although suffix trees are fast to query and build, they are costly to <lb/>store. An alternative to the suffix tree is the suffix array, a sorted array <lb/>of all suffixes that are able to perform all tasks that a suffix tree can <lb/>perform in the same time when &apos;enhanced&apos; with a Longest Common <lb/>Prefix (LCP) array [1]. The LCP array gives for each adjacent pair of <lb/>suffixes, the longest common prefix shared between them. For exam-<lb/>ple, in Fig. 5, the longest common prefix between positions 6 and 7 <lb/>in the suffix array, or &apos;bca&apos; and &apos;cabbca&apos; is 2, referring to the length <lb/>of &apos;ca&apos; common to both suffixes. From this LCP array in combina-<lb/>tion with the suffix array, [1] showed that all suffix tree traversal types <lb/>could be supported. <lb/>The algorithm, defined below takes a collection of time series ap-<lb/>proximations S approx  , finds all the common patterns, then returns a <lb/>filtered list of FLPs matching the criteria defined in Section 4.2.2. <lb/> 1:  procedure DETECT FLPS(S  approx  ) <lb/> 2: <lb/> Set FLPs  ← {} <lb/> 3: <lb/> for all S i  ∈ S approx  do <lb/> 4: <lb/> Array su f f ix  ← sort(createSuffixArray(S  i  )) <lb/> 5: <lb/> Array lcp  ← createLCPArray(Array  su f f ix  ) <lb/> 6: <lb/> pos  1  ← 0 <lb/> 7: <lb/> index ← 0 <lb/> 8: <lb/> while index &lt; Array lcp  .length do <lb/> 9: <lb/> pos  2  ← Array su f f ix  [index + 1] <lb/> 10: <lb/> length ← Array lcp  [index] <lb/> 11: <lb/> end index  ← max(pos1, pos2) + length <lb/> 12: <lb/> FLP candidate  ←getSuffix(S  i  , index, end index  ) <lb/> 13: <lb/> if FLP candidate  ∃Set  FLPs  then <lb/> 14: <lb/> get(Set  FLPs  , FLP candidate  ).occurrence++ <lb/> 15: <lb/> else <lb/> 16: <lb/> FLP candidate  .occurrence ← 1 <lb/> 17: <lb/> Set FLPs  ← FLP candidate <lb/> 18: <lb/> end if <lb/> 19: <lb/> index ← index + 1 <lb/> 20: <lb/> pos1 ← pos2 <lb/> 21: <lb/> end while <lb/> 22: <lb/> end for <lb/> 23: <lb/> return Filter FLPS(Set  FLPs  ) <lb/> see 4.2.2 <lb/> 24:  end procedure <lb/> 4.2.2 Selecting the &apos;Best&apos; FLPs <lb/> Let Ξ be an FLP found using the detection algorithm in Section 4.2.1, <lb/>which can be written as a series of letters Ξ = α  1  α  2  . . . α  m  . L  Ξ  denotes <lb/>its length (i.e., the number of letters) and F  Ξ  denotes its frequency of <lb/>occurrence obtained by the detection algorithm. F  Ξ  can be the num-<lb/>ber of occurrence of Ξ in the data repository, or be moderated by a <lb/>predefined maximum number. The compression ratio for Ξ is <lb/> C(Ξ) = <lb/> ω  s  × N spu  × L  Ξ <lb/> ω  g <lb/> (4) <lb/>In Eq. (4), ω  s  is the minimal width along the t-axis required to ade-<lb/>quately distinguish a data point in a time series (e.g., 2 pixels), ω  g  be <lb/>the screen width of a glyph, and N spu  is the window size for the sym-<lb/>bolic approximation (i.e., the number of samples per letter). As the <lb/>actual realization of this compression ratio depends on the probability <lb/>of its occurrence in an input string, we thus moderate C(Ξ) with F  Ξ  as <lb/> P C  (Ξ) = F  Ξ  ×C(Ξ) <lb/> (5) <lb/>We refer to P C  (Ξ) as the potential compression power of Ξ. For <lb/>example, consider a 20-letter string and Ξ, N spu  = 10. This FLP has <lb/>200 samples. If ω  s  = 2 pixels, and ω  g  = 40 pixels, we have C(Ξ) = <lb/> 2×10×20 <lb/>40 <lb/> = 10. This implies that the glyph replacement will take up <lb/>10% of the original space requirement for the 200 samples. Assuming <lb/>an unmoderated F  Ξ  = 50, we have P C  (Ξ) = 500. <lb/>The key balance to be found in the approach is finding the opti-<lb/>mal number for N spu  for any given dataset so that the system is able <lb/>to compress optimally and intelligently. A larger number for N spu  will <lb/>lead to a coarser representation of the time series, however it will prob-<lb/>abilistically lead to further rates of compression. Conversely, a small <lb/>number for N spu  will result in a more fine grained approximation for <lb/>all time series but lead to less compression due to a generally higher <lb/>amount of entropy in the approximation. <lb/> The selection of N  glyphs  &apos;best&apos; FLPs relies mainly on the P C  scores. <lb/>This may depend on a predefined limit of N glyphs  , as too many glyphs <lb/>would incur extra difficulties in learning, recognition and memoriza-<lb/>tion. This may alternatively depend on a preset threshold for P C  . In <lb/>the simplest case, the process can stop here and the results can be com-<lb/>piled into a &apos;dictionary&apos; of FLPs D. The resulting dictionary can then <lb/>be used to compress a time series (see Section 7). <lb/>In some cases however, an FLP may classify what can be deemed <lb/>an anomaly along with normal patterns due to subtle differences in <lb/>the series even though the overall shape of the distribution is similar. <lb/>This is where the visual analytics platform comes in to play whereby <lb/>users can view the feature space of an FLP (mean, standard deviation, <lb/>kurtosis, etc.) can sub classify an FLP and its corresponding time <lb/>series with more fine-grained control. <lb/> 5 FEATURE SPACE <lb/> As mentioned earlier, The FLP detection subsystem in Section 4 fo-<lb/>cuses on noise tolerance. Each of the selected FLPs determines an <lb/>individualized model, Ξ, which is defined in the resolution of the unit <lb/>encoding. This represents a significant approximation in terms of both <lb/>temporal range and attribute value range. A selected FLP can eas-<lb/>ily encompass many similar patterns of the time series. To be able to <lb/>distinguish between those time series segments that are &apos;valid&apos; versus <lb/>those that are not, we need the visual analytics platform to refine the <lb/>model. First, we apply the individualized model, Ξ, to all or a subset <lb/>of time series in the corpus. This would result in a collection of re-<lb/>sults, R = {R  1  , R  2  , . . . , R m  }, detected by using Ξ. We then construct a <lb/>feature space for R. <lb/> There are a variety of computable attributes for time series data used <lb/>in the literature (e.g., 23 considered by Tam et al [41]). We consider <lb/>a number of these variables to characterize each motif and the times <lb/>series data they represent. <lb/>Alongside more trivial metrics such as maximum and minimum val-<lb/>ues, average, standard deviation and variance, the software also cal-<lb/>culates the following: <lb/>1. Correlation/error: calculated as the Pearson product-moment <lb/>correlation coefficient between the approximation values and the <lb/>original time series values as a way of identifying how close <lb/>the approximation is to the reality. This coefficient is defined <lb/>in Equation 6 where X and Y are two arrays representing the <lb/>approximated series and the time series respectively, and ¯ <lb/> X, ¯ <lb/> Y <lb/> represent the mean of both arrays. <lb/> ρ = <lb/> ∑ <lb/> n <lb/>i=1  (X  i  − ¯ <lb/> X)(Y i  − ¯ <lb/> Y ) <lb/> ∑ <lb/> n <lb/>i=1  (X  i  − ¯ <lb/> X)  2 <lb/> ∑ <lb/> n <lb/>i=1  (Y  i  − ¯ <lb/> Y )  2 <lb/> (6) <lb/>2. Kurtosis: kurtosis, as illustrated in Fig. 6 gives a measure of <lb/>the &apos;peakedness&apos; of a distribution. This is calculated using the <lb/>equation defined in Equation 7 where X is the original time series <lb/> T . <lb/> κ = <lb/> 1 <lb/> nσ  4 <lb/> X <lb/>n <lb/> ∑ <lb/> i=1 <lb/> (X  i  − ¯ <lb/> X) <lb/> 4 <lb/> (7) <lb/>3. Skewness: the skewness, illustrated in Fig. 6 gives a measure of <lb/>&apos;curve asymmetry&apos; [41] and is defined in Equation 8. <lb/> γ = <lb/> 1 <lb/> nσ  3 <lb/> X <lb/>n <lb/> ∑ <lb/> i=1 <lb/> (X  i  − ¯ <lb/> X) <lb/> 3 <lb/> (8) <lb/>4. Burstiness: also called local variance, indicates how quick ad-<lb/>jacent values rise or fall. The equation below has been adapted <lb/>from [39]. <lb/> L V  = <lb/> 1 <lb/> n − 1 <lb/> n−1 <lb/> ∑ <lb/> i=1 <lb/> 3(X  i  − X i+1  )  2 <lb/> (X  i  + X i+1  )  2 <lb/> (9) <lb/> (+) Leptokurtic <lb/>(0) Mesokurtic <lb/>(-) Platykurtic <lb/>(+) Positively skewed <lb/>(-) Negatively skewed <lb/> 0.34 <lb/> 0.54 <lb/>0.74 <lb/> Repeat count <lb/>Approximation <lb/>Actual series <lb/>κ -Kurtosis <lb/>γ Skewness <lb/> 0.79 <lb/> σ Std. Deviation <lb/>ρ Correlation <lb/> Correlated <lb/>Not correlated <lb/> Window Size <lb/> # grey lines <lb/> ...more features can <lb/>be added <lb/> Fig. 6. A) The compression power of a particular FLP and the part of <lb/>the time series compressed by a glyph is given by an overview bar. <lb/>B) Details are available on demand when users click on a glyph in an <lb/>implementation inspired by StackZooming [21]. <lb/> Additionally, each motif has a compression potential calculated as <lb/>part of the FLP algorithm discussed in Section 4.2. <lb/>Similar to Tam et al [41], we visualize these parameters using par-<lb/>allel coordinate plots (see Fig. 2D) and provide interaction to allow <lb/>users to edit the model. Additionally, these features can be incorpo-<lb/>rated in to glyphs as illustrated in Fig. 6 and visualized for the user. <lb/> 6 MODEL EDITING <lb/> The process of model editing is enabled through many components <lb/>within the visual analytics platform shown in Fig. 7 A and B. Users <lb/>can click on nodes in the network view and accept or reject individual <lb/>motifs whilst seeing the effect of such refinements immediately. The <lb/>model editing window, shown in more detail in Fig. 7 A2 and B2 can <lb/>be split in to two sections: 1) the model panel; and 2) the refinement <lb/>panel. <lb/>The model panel is there to support the generation of motifs with <lb/>differing window and alphabet sizes for the SAX algorithm used for <lb/>FLP detection. These models are combined together and the combina-<lb/>tion of model outputs provides the total number of motifs available for <lb/>selection. <lb/>The refinement panel supports the acceptance or rejection of motifs <lb/>based on one or more of the parameters in the feature space. Users can <lb/>combine these operations with a number of logical operations such as <lb/>union (OR), intersection (AND) and subtract (SUB). Users can also <lb/>specify whether values should lay within a range or be less than, less <lb/>than or equal to, greater than or greater than or equal to some limit <lb/>value. <lb/>The combination of simple logic statements provides a way for <lb/>users to visually program the model so that it outputs only the desired <lb/>results. <lb/>Fig. 7 shows an overview of the visual analytics workflow applied <lb/>for the engineering example depicting the solenoid current measure-<lb/>ments on a Marotta valve used to control fuel on a space shuttle. In <lb/>Fig. 7 A, a network view displays the motifs found by the FLP detec-<lb/>tion algorithm as glyphs. Hovering over a glyph highlights the area <lb/>that glyph represents in the time series corpus. This shows that one <lb/>FLP represents all of patterns in the time series corpus, however an <lb/>expert (derived from annotations in the original data set) marked the <lb/>FLP highlighted in the black outline as anomalous. The next step will <lb/>involve filtering this pattern out by looking in more detail at the FLPs <lb/>and the time series patterns they represent. <lb/>Fig. 7 B shows a view of all 16 occurrences of the FLP of interest <lb/>from the first step. Each occurrence has its own feature space repre-<lb/>senting the actual series represented. We have clicked on the motif of <lb/>interest and can see in the popup that the profile matches the anomaly <lb/>we wish to exclude. <lb/> E. Compressed Series with anomalies in place <lb/>A. FLP Detection Results <lb/>B. View FLP in detail with individual occurrences <lb/> Anomalous Region Preserved <lb/> D. FLP Selection -arranged with the &apos;distance&apos; layout -only those marked are used <lb/> This FLP is covering too much of <lb/>the time series including what the <lb/>domain expert sees as an anomaly <lb/> C. Find the features that lter the anomalous FLP from the desired results <lb/> The mean on this occasion <lb/>was the best feature for <lb/>separating the valid and <lb/>invalid motifs <lb/> Fig. 7. A visual analytics approach to model refinement on an engineering time series depicting the solenoid current measurements on a Marotta <lb/>valve used to control fuel on a space shuttle [14]. <lb/> Having identified the anomalous motif, Fig. 7 C shows how, <lb/>through use of the parallel coordinates, we can find the features that <lb/>are able to classify between valid and invalid FLPs. In this case, the <lb/>mean was a good choice since the anomalous FLP had a mean much <lb/>lower than the others. The network layout automatically updates to <lb/>visually separate the rejected FLPs from those that were accepted. In <lb/>D, the set of FLPs are then exported as a dictionary where feature <lb/>refinements are also preserved for later compression. Finally, a time <lb/>series can be compressed using the FLPs in the dictionary and some <lb/>time series T to create a compressed representation of the time series <lb/>with the anomalous region preserved. This visual compression step is <lb/>described more in the next section. <lb/> 7 VISUALIZING THE COMPRESSED TIME SERIES <lb/> Given a dictionary of FLPs D obtained from the FLP Detection Sub-<lb/>system (Section 4.2), the process of glyph-based time series compres-<lb/>sion, illustrated in Fig. 8, can be applied repeatedly to many time se-<lb/>ries. Using traffic signage as an analog, once functional and visual rep-<lb/>resentations of different signs have been decided, we can place them <lb/>in any applicable location. <lb/>Glyph-based time series compression involves two steps: 1) given a <lb/>dictionary of FLPs D and a time series T , create a compressed repre-<lb/>sentation of a time series; and 2) using this representation of the time <lb/>series, render it. <lb/>For the first step, we create the enhanced suffix array representa-<lb/>tion as performed when creating D. From this data structure, a binary <lb/>search is performed for each of the FLPs defined in D to identify all <lb/>positions (defined by the suffix array) the particular FLP appears in. <lb/>From the constructed list of matching positions for each FLP, some <lb/>additional processing is made for the rendering layer. The first is <lb/>to merge directly adjacent matches, that is, say one of our FLPs is <lb/>composed of abcd, then for abcdabcdddd, . . ., we have a record of <lb/> Ξ  abcd  [1, 5] including a list of matching indexes. Here the opera-<lb/>tor  denotes annotation, meaning that a FLP Ξ of symbols abcd is <lb/>annotated with index values 1 and 5. Since the distances in the start <lb/>indexes for the occurrences of Ξ  abcd  are separated exactly by its length <lb/> L  Ξ  , 1 + L  Ξ  = 5, the second occurrence can be merged with the first, <lb/>and a &apos;repeat count&apos; for Ξ  abcd  is increased, so Ξ  abcd  [1, R2]. Finally, <lb/>a further check is performed to ensure that the indexes for the matched <lb/>FLPs do not overlap, that is if Ξ  bcd  [2, 6] has start indexes overlap-<lb/>ping with Ξ  abcd  [1, R2]. In this case, the FLP with the lowest com-<lb/>pression ratio (C) power will be removed from the results. At the end <lb/>of this process, the software produces a map from each start index of <lb/>a FLP to its symbolic representation and repeat count. <lb/>The second step, rendering the time series, involves the incorpora-<lb/>tion of the glyphs representing compressed regions in to a time series <lb/>visualization, the design of which is shown in Fig. 9. This representa-<lb/>tion maintains an overview bar showing the overall length of the time <lb/>series whilst showing how much of a region has been compressed by <lb/>a glyph. The glyphs provide information about the approximation, the <lb/>run length (how many times the motif was repeated) and a number <lb/>of metrics from the feature space. Glyphs can be interacted with and <lb/>expanded to provide details of the underlying compressed time series <lb/>represented by the glyph. This can be seen in Fig. 7 A and B. Addi-<lb/></body>

			<note place="headnote"> FLP Dictionary and <lb/>Glyph Set <lb/></note>

			<body>1. Time Series <lb/>3. Find FLPs in <lb/>Time Series <lb/>5. Return compressed <lb/>series <lb/> abbcbabbbbbbabcc <lb/> Sequence <lb/>matching <lb/> 2. Create symbolic encoding <lb/> Aggregation <lb/> Unit <lb/>Encoding <lb/>4. Visual Compression <lb/> Visual <lb/>Compression <lb/> Fig. 8. Dictionary-based compression steps – Given a time series T  1  , we encode it symbolically (2). Then, with the FLP dictionary created in (A), an <lb/>algorithm finds the occurring FLPs in the incoming sequence (3). These found FLPs are replaced with glyphs (4) and the compressed time series <lb/>is visualized. <lb/> 0.34 <lb/> 0.54 <lb/>0.74 <lb/>0.79 <lb/>0.34 <lb/>0.64 <lb/>0.74 <lb/>0.79 <lb/>0.34 <lb/>0.54 <lb/>0.74 <lb/>0.79 <lb/> Length n Repeats 3 <lb/> B) Glyph details available on demand <lb/>A) Compression size put in perspective with overall series width <lb/> Fig. 9. A) The compression power of a particular FLP and the part of <lb/>the time series compressed by a glyph is given by an overview bar. <lb/>B) Details are available on demand when users click on a glyph in an <lb/>implementation inspired by StackZooming [21]. <lb/> tionally, when large areas are compressed, this changes the rendering <lb/>of a time series somewhat since the series can spread over a much <lb/>larger range. In many cases this is acceptable, but in some cases, the <lb/>spacing may introduce interpretation problems. To navigate this issue, <lb/>users can select to maintain the aspect ratio of the original series. <lb/> 8 SOFTWARE AVAILABILITY <lb/> The software will be open source (on publication) and has been de-<lb/>veloped in a modular way to enable reuse at any level of the soft-<lb/>ware stack. Each of the three components, the: FLP detection subsys-<lb/>tem, implemented in Python; visual analytics platform, implemented <lb/>as a thick client built with HTML5, CSS and JavaScript (with D3 and <lb/>jQuery); and library to visualize the results, also built with HTML5, <lb/>CSS and JavaScript (with D3 and jQuery) are available separately and <lb/>can be run independently. The visual analytics platform and the com-<lb/>pressed series output code rely on a simple JSON (JavaScript Object <lb/>Notation) serialized format while the FLP detection subsystem outputs <lb/>its results in the format required by the front end components. <lb/>Due to the modularity in the code base, and in particular the visual <lb/>analytics platform, we envisage the software being of use in model <lb/>refinement for similar problems requiring a visual analytics approach. <lb/> 9 CONCLUSIONS AND FUTURE WORK <lb/> In this work, we utilized the capabilities of visual analytics to address <lb/>a challenging problem in time series analysis and visualization. In <lb/>order to refine a model derived from the traditional FLP algorithm, <lb/>visual analytics acted as a &apos;software development tool&apos; by enabling <lb/>users to test the model, analyze and visualize its results in a feature <lb/>space, identify false positive results, identify related parameter ranges, <lb/>and edit the model accordingly. <lb/>Because the feature space is computed directly with the original <lb/>time series data, it can capture finer feature differences between an <lb/>anomaly and its closely related FLP. Because the computation is only <lb/>for a limited set of test results, it is also cost-effective. Most impor-<lb/>tantly, this allows human users to have the direct control over the tasks <lb/>of debugging the results from a model and selecting features for refin-<lb/>ing a model. <lb/>In our future work, we would like to develop new visual represen-<lb/>tations of model space, which can support model visualization and <lb/>editing in a semantically more meaningful manner. <lb/></body>

			<listBibl> REFERENCES <lb/> [1] M. I. Abouelhoda, S. Kurtz, and E. Ohlebusch. Replacing suffix trees <lb/>with enhanced suffix arrays. Journal of Discrete Algorithms, 2(1):53–86, <lb/>2004. <lb/>[2] W. Aigner, S. Miksch, W. Mller, H. Schumann, and C. Tominski. Visual-<lb/>izing time-oriented data — A systematic view. Computers &amp; Graphics, <lb/> 31(3):401 – 409, 2007. <lb/>[3] W. Aigner, S. Miksch, B. Thurnher, and S. Biffl. Planninglines: novel <lb/>glyphs for representing temporal uncertainties and their evaluation. pages <lb/>457–463, 2005. <lb/>[4] D. J. Berndt and J. Clifford. Using dynamic time warping to find patterns <lb/>in time series. In KDD workshop, volume 10, pages 359–370. Seattle, <lb/>WA, 1994. <lb/>[5] M. Bögl, W. Aigner, P. Filzmoser, T. Lammarsch, S. Miksch, and A. Rind. <lb/>Visual analytics for model selection in time series analysis. IEEE Trans-<lb/>actions on Visualization and Computer Graphics, Special Issue &quot; VIS <lb/>2013 &quot; , 19, 12/2013 2013. <lb/>[6] S. H. Cameron. Piece-wise linear approximations. Technical report, <lb/>DTIC Document, 1966. <lb/>[7] K. Chan and A. W. Fu. Efficient time series matching by wavelets. pages <lb/>126–133, 1999. <lb/>[8] K.-P. Chan and A. W.-C. Fu. Efficient time series matching by wavelets. <lb/>In Data Engineering, 1999. Proceedings., 15th International Conference <lb/>on, pages 126–133. IEEE, 1999. <lb/>[9] L. Chen, M. T. ¨ <lb/>Ozsu, and V. Oria. Robust and fast similarity search for <lb/>moving object trajectories. In Proceedings of the 2005 ACM SIGMOD <lb/>international conference on Management of data, pages 491–502. ACM, <lb/>2005. <lb/>[10] M. Chen and H. Jaenicke. An information-theoretic framework for visu-<lb/>alization. Visualization and Computer Graphics, IEEE Transactions on, <lb/> 16(6):1206–1215, 2010. <lb/>[11] Y. Drocourt, R. Borgo, K. Scharrer, T. Murray, S. I. Bevan, and M. Chen. <lb/>Temporal visualization of boundary-based geo-information using radial <lb/>projection. Computer Graphics Forum, (3):981990, 2011. <lb/>[12] C. Faloutsos, M. Ranganathan, and Y. Manolopoulos. Fast subsequence <lb/>matching in time-series databases. volume 23. ACM, 1994. <lb/>[13] S. Fernandes Silva and T. Catarci. Visualization of linear time-oriented <lb/>data: a survey. In Web Information Systems Engineering, 2000. Proceed-<lb/>ings of the First International Conference on, volume 1, pages 310–319 <lb/>vol.1, 2000. <lb/>[14] B. Ferrell and S. Santuro. <lb/>Nasa shuttle valve data. <lb/>http://www.cs.fit.edu/ pkc/nasa/data/ ., 2005. <lb/> [15] J. Fuchs, F. Fischer, F. Mansmann, E. Bertini, and P. Isenberg. Evaluation <lb/>of alternative glyph designs for time series data in a small multiple setting. <lb/>2013. <lb/>[16] M. C. Hao, U. Dayal, D. A. Keim, and T. Schreck. Importance-driven <lb/>visualization layouts for large time series data. pages 203–210, 2005. <lb/>[17] M. C. Hao, U. Dayal, D. A. Keim, and T. Schreck. Multi-resolution <lb/>techniques for visual exploration of large time-series data. pages 27–34, <lb/>2007. <lb/>[18] M. C. Hao, M. Marwah, H. Janetzko, U. Dayal, D. A. Keim, D. Patnaik, <lb/>N. Ramakrishnan, and R. K. Sharma. Visual exploration of frequent pat-<lb/>terns in multivariate time series. Information Visualization, 11(1):71–83, <lb/>2012. <lb/>[19] J. Heer, N. Kong, and M. Agrawala. Sizing the horizon: The effects of <lb/>chart size and layering on the graphical perception of time series visual-<lb/>izations. In In Proc. ACM Human Factors in Computing Systems (CHI, <lb/> pages 1303–1312, 2009. <lb/>[20] H. Hochheiser and B. Shneiderman. Interactive exploration of time series <lb/>data. In Discovery Science, pages 441–446. Springer, 2001. <lb/>[21] W. Javed and N. Elmqvist. Stack zooming for multi-focus interaction in <lb/>time-series data visualization. pages 33–40, 2010. <lb/>[22] T. Kasai, G. Lee, H. Arimura, S. Arikawa, and K. Park. Linear-time <lb/>longest-common-prefix computation in suffix arrays and its applications. <lb/>In Combinatorial Pattern Matching, pages 181–192. Springer, 2001. <lb/>[23] D. A. Keim, T. Nietzschmann, N. Schelwies, J. Schneidewind, <lb/>T. Schreck, and H. Ziegler. A spectral visualization system for analyz-<lb/>ing financial time series data. In Proceedings of the Eighth Joint Euro-<lb/>graphics / IEEE VGTC conference on Visualization, EUROVIS&apos;06, pages <lb/>195–202, 2006. <lb/>[24] E. Keogh, K. Chakrabarti, M. Pazzani, and S. Mehrotra. Dimensionality <lb/>reduction for fast similarity search in large time series databases. Knowl-<lb/>edge and information Systems, 3(3):263–286, 2001. <lb/>[25] E. Keogh, K. Chakrabarti, M. Pazzani, and S. Mehrotra. Locally adap-<lb/>tive dimensionality reduction for indexing large time series databases. In <lb/> ACM SIGMOD Record, volume 30, pages 151–162. ACM, 2001. <lb/>[26] E. Keogh and S. Kasetty. On the need for time series data mining bench-<lb/>marks: a survey and empirical demonstration. Data Mining and Knowl-<lb/>edge Discovery, 7(4):349–371, 2003. <lb/>[27] E. Keogh, J. Lin, and A. Fu. <lb/>Time series datasets, <lb/>http://www.cs.ucr.edu/ eamonn/discords/. <lb/>[28] E. Keogh, J. Lin, and A. Fu. Hot sax: Efficiently finding the most un-<lb/>usual time series subsequence. In Data mining, fifth IEEE international <lb/>conference on, pages 8–pp. IEEE, 2005. <lb/>[29] E. Keogh, L. Wei, X. Xi, S. Lonardi, J. Shieh, and S. Sirowy. Intelligent <lb/>icons: Integrating lite-weight data mining and visualization into gui op-<lb/>erating systems. In Data Mining, 2006. ICDM &apos;06. Sixth International <lb/>Conference on, pages 912–916, 2006. <lb/>[30] R. Kincaid. SignalLens: Focus+Context applied to electronic time series. <lb/> IEEE transactions on visualization and computer graphics, 16(6):900– <lb/>907, 2010. <lb/>[31] W. Lang, M. Morse, and J. M. Patel. Dictionary-based compression <lb/>for long time-series similarity. Knowledge and Data Engineering, IEEE <lb/>Transactions on, 22(11):1609–1622, 2010. <lb/>[32] P. Legg, D. Chung, M. Parry, R. Bown, M. Jones, I. Griffiths, and <lb/>M. Chen. Transformation of an uncertain video search pipeline to a <lb/>sketch-based visual analytics loop. IEEE transactions on visualization <lb/>and computer graphics, 19(12):2109–2118, 2013. <lb/>[33] J. Lin, E. Keogh, and S. Lonardi. Visualizing and discovering non-<lb/>trivial patterns in large time series databases. Information visualization, <lb/> 4(2):61–82, 2005. <lb/>[34] J. Lin, E. Keogh, S. Lonardi, and B. Chiu. A symbolic representation of <lb/>time series, with implications for streaming algorithms. DMKD, 2003. <lb/>[35] J. Lin, E. Keogh, L. Wei, and S. Lonardi. Experiencing SAX: a novel <lb/>symbolic representation of time series. Data Mining and Knowledge Dis-<lb/>covery, 15, 2007. <lb/>[36] P. McLachlan, T. Munzner, E. Koutsofios, and S. North. LiveRAC: inter-<lb/>active visual exploration of system management time-series data. In Pro-<lb/>ceeding of the twenty-sixth annual SIGCHI conference on Human factors <lb/>in computing systems, page 14831492, 2008. <lb/>[37] M. D. Morse and J. M. Patel. An efficient and accurate method for eval-<lb/>uating time series similarity. In Proceedings of the 2007 ACM SIGMOD <lb/>international conference on Management of data, pages 569–580. ACM, <lb/>2007. <lb/>[38] P. Saraiya, P. Lee, and C. North. Visualization of graphs with associated <lb/>timeseries data. In Proc. IEEE Information Visualization, pages 225–232, <lb/>2005. <lb/>[39] S. Shinomoto, K. Shima, and J. Tanji. Differences in spiking patterns <lb/>among cortical neurons. Neural Computation, 15(12):2823–2842, 2003. <lb/>[40] B. Shneiderman. The eyes have it: A task by data type taxonomy for in-<lb/>formation visualizations. In Visual Languages, 1996. Proceedings., IEEE <lb/>Symposium on, pages 336–343. IEEE, 1996. <lb/>[41] G. K. Tam, H. Fang, A. J. Aubrey, P. W. Grant, P. L. Rosin, D. Marshall, <lb/>and M. Chen. Visualization of time-series data in parameter space for un-<lb/>derstanding facial dynamics. In Computer Graphics Forum, volume 30, <lb/>pages 901–910. Wiley Online Library, 2011. <lb/>[42] E. Ukkonen. On-line construction of suffix trees. Algorithmica, <lb/> 14(3):249–260, 1995. <lb/>[43] M. Vlachos, M. Hadjieleftheriou, D. Gunopulos, and E. Keogh. Index-<lb/>ing multi-dimensional time-series with support for multiple distance mea-<lb/>sures. In Proceedings of the ninth ACM SIGKDD international confer-<lb/>ence on Knowledge discovery and data mining, pages 216–225. ACM, <lb/>2003. <lb/>[44] M. O. Ward and Z. Guo. Visual exploration of time-series data with shape <lb/>space projections. In Computer Graphics Forum, volume 30, pages 701– <lb/>710. Wiley Online Library, 2011. <lb/>[45] M. Weber, M. Alexa, and W. Müller. Visualizing time-series on spirals. <lb/>In Infovis, volume 1, pages 7–14, 2001. <lb/>[46] P. Weiner. Linear pattern matching algorithms. In Switching and Au-<lb/>tomata Theory, 1973. SWAT&apos;08. IEEE Conference Record of 14th Annual <lb/>Symposium on, pages 1–11. IEEE, 1973. <lb/>[47] H. Wickham, H. Hofmann, C. Wickham, and D. Cook. Glyph-maps for <lb/>visually exploring temporal patterns in climate data and models. Envi-<lb/>ronmetrics, 23(5):382–393, 2012. <lb/>[48] J. Zhao, F. Chevalier, E. Pietriga, and R. Balakrishnan. Exploratory analy-<lb/>sis of time-series with ChronoLenses. IEEE transactions on visualization <lb/>and computer graphics, 17(12):2422–2431, 2011. </listBibl>


	</text>
</tei>
